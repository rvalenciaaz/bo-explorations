{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8f0b2-e8ab-448c-ba7c-e49a1a1987d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c552965-d1b8-4e4c-9144-cae557259e37",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     bic \u001b[38;5;241m=\u001b[39m n \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(mse) \u001b[38;5;241m+\u001b[39m n_params \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bic\n\u001b[0;32m---> 44\u001b[0m bic_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43m[\u001b[49m\u001b[43mcompute_bic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbase_learners\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Step 5: Compute posterior probabilities\u001b[39;00m\n\u001b[1;32m     47\u001b[0m log_marginal_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m bic_scores\n",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m     bic \u001b[38;5;241m=\u001b[39m n \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(mse) \u001b[38;5;241m+\u001b[39m n_params \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bic\n\u001b[0;32m---> 44\u001b[0m bic_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mcompute_bic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m base_learners])\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Step 5: Compute posterior probabilities\u001b[39;00m\n\u001b[1;32m     47\u001b[0m log_marginal_likelihoods \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m bic_scores\n",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m, in \u001b[0;36mcompute_bic\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bic\u001b[39m(model, X, y):\n\u001b[1;32m     30\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X, y)\n\u001b[0;32m---> 31\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Number of parameters estimation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch_mar2024/lib/python3.11/site-packages/sklearn/svm/_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    429\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    430\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch_mar2024/lib/python3.11/site-packages/sklearn/svm/_base.py:450\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    446\u001b[0m         )\n\u001b[1;32m    448\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[0;32m--> 450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_new, y_train, y_new = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize base learners\n",
    "model_1 = BayesianRidge()\n",
    "model_2 = DecisionTreeRegressor(random_state=42)\n",
    "model_3 = SVR()\n",
    "base_learners = [model_1, model_2, model_3]\n",
    "\n",
    "# Step 3: Assign equal prior probabilities\n",
    "priors = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Step 4: Compute BIC scores for each model\n",
    "def compute_bic(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    # Number of parameters estimation\n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_params = len(model.coef_)\n",
    "    elif isinstance(model, DecisionTreeRegressor):\n",
    "        n_params = model.tree_.node_count\n",
    "    else:\n",
    "        n_params = len(model.support_)\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    n = len(y)\n",
    "    bic = n * np.log(mse) + n_params * np.log(n)\n",
    "    return bic\n",
    "\n",
    "bic_scores = np.array([compute_bic(model, X_train, y_train) for model in base_learners])\n",
    "\n",
    "# Step 5: Compute posterior probabilities\n",
    "log_marginal_likelihoods = -0.5 * bic_scores\n",
    "log_unnormalized_posteriors = log_marginal_likelihoods + np.log(priors)\n",
    "log_posterior_probabilities = log_unnormalized_posteriors - np.logaddexp.reduce(log_unnormalized_posteriors)\n",
    "posterior_probabilities = np.exp(log_posterior_probabilities)\n",
    "\n",
    "print(\"Posterior Probabilities:\")\n",
    "for i, prob in enumerate(posterior_probabilities):\n",
    "    print(f\"Model {i+1}: {prob:.4f}\")\n",
    "\n",
    "# Step 6: Make predictions on new data\n",
    "predictions = np.array([model.predict(X_new) for model in base_learners])\n",
    "\n",
    "# Step 7: Compute BMA prediction\n",
    "bma_prediction = np.dot(posterior_probabilities, predictions)\n",
    "\n",
    "# Evaluate the BMA prediction\n",
    "mse_bma = mean_squared_error(y_new, bma_prediction)\n",
    "print(f\"\\nBMA Prediction MSE: {mse_bma:.4f}\")\n",
    "\n",
    "# Compare with individual models\n",
    "for i, model in enumerate(base_learners):\n",
    "    mse_model = mean_squared_error(y_new, predictions[i])\n",
    "    print(f\"Model {i+1} MSE: {mse_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cfe7e4-6e43-4224-81f9-324e52507280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load and Prepare the Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X_train, X_new, y_train, y_new = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Define BoTorch Models as Base Learners\n",
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, MaternKernel, LinearKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "model_1 = SingleTaskGP(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    covar_module=ScaleKernel(RBFKernel()),\n",
    ")\n",
    "model_2 = SingleTaskGP(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    covar_module=ScaleKernel(MaternKernel(nu=2.5)),\n",
    ")\n",
    "model_3 = SingleTaskGP(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    covar_module=ScaleKernel(LinearKernel()),\n",
    ")\n",
    "base_learners = [model_1, model_2, model_3]\n",
    "\n",
    "# Step 3: Assign Prior Probabilities\n",
    "priors = np.array([1/3, 1/3, 1/3])\n",
    "\n",
    "# Step 4: Compute Marginal Likelihoods\n",
    "def compute_log_marginal_likelihood(model, X, y):\n",
    "    model.train()\n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    training_iterations = 50\n",
    "    for _ in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = -mll(output, y).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        log_marg_likelihood = mll(model(X), y).item()\n",
    "    return log_marg_likelihood\n",
    "\n",
    "log_marginal_likelihoods = np.array([\n",
    "    compute_log_marginal_likelihood(model, X_train_tensor, y_train_tensor)\n",
    "    for model in base_learners\n",
    "])\n",
    "\n",
    "# Step 5: Compute Posterior Model Probabilities\n",
    "log_priors = np.log(priors)\n",
    "log_unnormalized_posteriors = log_marginal_likelihoods + log_priors\n",
    "max_log_post = np.max(log_unnormalized_posteriors)\n",
    "log_unnormalized_posteriors -= max_log_post  # For numerical stability\n",
    "unnormalized_posteriors = np.exp(log_unnormalized_posteriors)\n",
    "posterior_probabilities = unnormalized_posteriors / np.sum(unnormalized_posteriors)\n",
    "\n",
    "print(\"Posterior Probabilities:\")\n",
    "for i, prob in enumerate(posterior_probabilities):\n",
    "    print(f\"Model {i+1}: {prob:.4f}\")\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "X_new_tensor = torch.tensor(X_new.values, dtype=torch.float32)\n",
    "\n",
    "def predict(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        posterior = model.posterior(X)\n",
    "        mean = posterior.mean.squeeze(-1)\n",
    "        variance = posterior.variance.squeeze(-1)\n",
    "    return mean.numpy(), variance.numpy()\n",
    "\n",
    "predictions_mean = []\n",
    "predictions_variance = []\n",
    "\n",
    "for model in base_learners:\n",
    "    mean, variance = predict(model, X_new_tensor)\n",
    "    predictions_mean.append(mean)\n",
    "    predictions_variance.append(variance)\n",
    "\n",
    "predictions_mean = np.array(predictions_mean)\n",
    "predictions_variance = np.array(predictions_variance)\n",
    "\n",
    "# Step 7: Compute the Bayesian Model Averaged Prediction\n",
    "bma_mean = np.dot(posterior_probabilities, predictions_mean)\n",
    "bma_variance = (\n",
    "    np.dot(posterior_probabilities, predictions_variance) +\n",
    "    np.dot(posterior_probabilities, (predictions_mean - bma_mean[None, :]) ** 2)\n",
    ")\n",
    "\n",
    "# Step 8: Evaluate the Models\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse_bma = mean_squared_error(y_new, bma_mean)\n",
    "print(f\"\\nBMA Prediction MSE: {mse_bma:.4f}\")\n",
    "\n",
    "for i, mean in enumerate(predictions_mean):\n",
    "    mse_model = mean_squared_error(y_new, mean)\n",
    "    print(f\"Model {i+1} MSE: {mse_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbf5d7a-23a0-454a-b977-aeca6001f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior Probabilities:\n",
      "Model 1: 0.0000\n",
      "Model 2: 1.0000\n",
      "Model 3: 0.0000\n",
      "Model 4: 0.0000\n",
      "\n",
      "BMA Prediction MSE: 0.4952\n",
      "Model 1 MSE: 0.5556\n",
      "Model 2 MSE: 0.4952\n",
      "Model 3 MSE: 1.3320\n",
      "Model 4 MSE: 0.2554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Step 1: Load and Prepare the Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X_train, X_new, y_train, y_new = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Define scikit-learn Models as Base Learners\n",
    "model_1 = BayesianRidge()\n",
    "model_2 = DecisionTreeRegressor(random_state=42)\n",
    "model_3 = SVR()\n",
    "model_4 = RandomForestRegressor(random_state=42)\n",
    "base_learners = [model_1, model_2, model_3, model_4]\n",
    "\n",
    "# Step 3: Assign Prior Probabilities\n",
    "priors = np.array([1/len(base_learners)] * len(base_learners))\n",
    "\n",
    "# Step 4: Compute BIC Scores\n",
    "def compute_bic(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    # Estimate the number of parameters\n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_params = len(model.coef_)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        n_params = np.count_nonzero(model.feature_importances_)\n",
    "    elif hasattr(model, 'n_support_'):\n",
    "        n_params = model.n_support_.sum()\n",
    "    else:\n",
    "        n_params = X.shape[1]  # Fallback to number of features\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    n = len(y)\n",
    "    bic = n * np.log(mse) + n_params * np.log(n)\n",
    "    return bic\n",
    "\n",
    "bic_scores = np.array([compute_bic(model, X_train, y_train) for model in base_learners])\n",
    "\n",
    "# Step 5: Compute Posterior Probabilities\n",
    "log_marginal_likelihoods = -0.5 * bic_scores\n",
    "log_priors = np.log(priors)\n",
    "log_unnormalized_posteriors = log_marginal_likelihoods + log_priors\n",
    "max_log_post = np.max(log_unnormalized_posteriors)\n",
    "log_unnormalized_posteriors -= max_log_post  # For numerical stability\n",
    "unnormalized_posteriors = np.exp(log_unnormalized_posteriors)\n",
    "posterior_probabilities = unnormalized_posteriors / np.sum(unnormalized_posteriors)\n",
    "\n",
    "print(\"Posterior Probabilities:\")\n",
    "for i, prob in enumerate(posterior_probabilities):\n",
    "    print(f\"Model {i+1}: {prob:.4f}\")\n",
    "\n",
    "# Step 6: Make Predictions\n",
    "predictions = np.array([model.predict(X_new) for model in base_learners])\n",
    "\n",
    "# Step 7: Compute BMA Prediction\n",
    "bma_prediction = np.dot(posterior_probabilities, predictions)\n",
    "\n",
    "# Step 8: Evaluate the Models\n",
    "mse_bma = mean_squared_error(y_new, bma_prediction)\n",
    "print(f\"\\nBMA Prediction MSE: {mse_bma:.4f}\")\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    mse_model = mean_squared_error(y_new, pred)\n",
    "    print(f\"Model {i+1} MSE: {mse_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6696532f-d461-42c4-9c4f-958b3b05177a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIC Scores and Number of Parameters:\n",
      "Model 1: BIC = -10785.6666, Number of Parameters = 8\n",
      "Model 2: BIC = -1177970.9512, Number of Parameters = 8\n",
      "Model 3: BIC = 152304.7942, Number of Parameters = 15149\n",
      "Model 4: BIC = -55123.4526, Number of Parameters = 8\n",
      "\n",
      "Posterior Probabilities:\n",
      "Model 1: 0.0000\n",
      "Model 2: 1.0000\n",
      "Model 3: 0.0000\n",
      "Model 4: 0.0000\n",
      "\n",
      "BMA Prediction MSE: 0.4952\n",
      "Model 1 MSE: 0.5556\n",
      "Model 2 MSE: 0.4952\n",
      "Model 3 MSE: 1.3320\n",
      "Model 4 MSE: 0.2554\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Step 1: Load and Prepare the Dataset\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "X_train, X_new, y_train, y_new = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Define scikit-learn Models as Base Learners\n",
    "model_1 = BayesianRidge()\n",
    "model_2 = DecisionTreeRegressor(random_state=42)\n",
    "model_3 = SVR()\n",
    "model_4 = RandomForestRegressor(random_state=42)\n",
    "base_learners = [model_1, model_2, model_3, model_4]\n",
    "\n",
    "# Step 3: Assign Prior Probabilities\n",
    "priors = np.array([1/len(base_learners)] * len(base_learners))\n",
    "\n",
    "# Step 4: Compute BIC Scores\n",
    "def compute_bic(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    # Estimate the number of parameters\n",
    "    if hasattr(model, 'coef_'):\n",
    "        n_params = len(model.coef_)\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        n_params = np.count_nonzero(model.feature_importances_)\n",
    "    elif hasattr(model, 'n_support_'):\n",
    "        n_params = model.n_support_.sum()\n",
    "    else:\n",
    "        n_params = X.shape[1]  # Fallback to number of features\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    n = len(y)\n",
    "    bic = n * np.log(mse) + n_params * np.log(n)\n",
    "    return bic, n_params\n",
    "\n",
    "# Step 5: Collect BIC Scores and Number of Parameters\n",
    "bic_scores = []\n",
    "n_params_list = []\n",
    "\n",
    "for model in base_learners:\n",
    "    bic, n_params = compute_bic(model, X_train, y_train)\n",
    "    bic_scores.append(bic)\n",
    "    n_params_list.append(n_params)\n",
    "\n",
    "bic_scores = np.array(bic_scores)\n",
    "\n",
    "# Step 6: Compute Posterior Probabilities\n",
    "log_marginal_likelihoods = -0.5 * bic_scores\n",
    "log_priors = np.log(priors)\n",
    "log_unnormalized_posteriors = log_marginal_likelihoods + log_priors\n",
    "max_log_post = np.max(log_unnormalized_posteriors)\n",
    "log_unnormalized_posteriors -= max_log_post  # For numerical stability\n",
    "unnormalized_posteriors = np.exp(log_unnormalized_posteriors)\n",
    "posterior_probabilities = unnormalized_posteriors / np.sum(unnormalized_posteriors)\n",
    "\n",
    "# Step 7: Print BIC Scores, Number of Parameters, and Posterior Probabilities\n",
    "print(\"BIC Scores and Number of Parameters:\")\n",
    "for i, (bic, n_params) in enumerate(zip(bic_scores, n_params_list)):\n",
    "    print(f\"Model {i+1}: BIC = {bic:.4f}, Number of Parameters = {n_params}\")\n",
    "\n",
    "print(\"\\nPosterior Probabilities:\")\n",
    "for i, prob in enumerate(posterior_probabilities):\n",
    "    print(f\"Model {i+1}: {prob:.4f}\")\n",
    "\n",
    "# Step 8: Make Predictions\n",
    "predictions = np.array([model.predict(X_new) for model in base_learners])\n",
    "\n",
    "# Step 9: Compute BMA Prediction\n",
    "bma_prediction = np.dot(posterior_probabilities, predictions)\n",
    "\n",
    "# Step 10: Evaluate the Models\n",
    "mse_bma = mean_squared_error(y_new, bma_prediction)\n",
    "print(f\"\\nBMA Prediction MSE: {mse_bma:.4f}\")\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    mse_model = mean_squared_error(y_new, pred)\n",
    "    print(f\"Model {i+1} MSE: {mse_model:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81325b-b203-484f-a181-710032662c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:botorch_mar2024]",
   "language": "python",
   "name": "conda-env-botorch_mar2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
